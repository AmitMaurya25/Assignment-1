{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aec173-0bae-4d2d-b913-603841486831",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: Designing a Pipeline for Feature Engineering and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "394fb170-dfbe-42f6-8223-42be33e24076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_0          0\n",
      "feature_1          0\n",
      "feature_2          0\n",
      "feature_3          0\n",
      "feature_4          0\n",
      "feature_5          0\n",
      "feature_6          0\n",
      "feature_7          0\n",
      "feature_8          0\n",
      "feature_9          0\n",
      "target_variable    0\n",
      "dtype: int64\n",
      "Accuracy: 0.855\n",
      "Cross-validation scores: [0.905 0.91  0.9   0.9   0.9  ]\n",
      "Mean CV accuracy: 0.9029999999999999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Convert to DataFrame\n",
    "data = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "data[\"target_variable\"] = y\n",
    "\n",
    "# Check missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Target variable\n",
    "target = data[\"target_variable\"]\n",
    "features = data.drop(\"target_variable\", axis=1)\n",
    "\n",
    "# Numerical and categorical column indices (in this synthetic example, we assume all columns are numerical)\n",
    "numerical_cols = list(range(data.shape[1] - 1))\n",
    "categorical_cols = []\n",
    "\n",
    "# Numerical pipeline\n",
    "numerical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline (we don't have categorical columns in this example, but you can add if needed)\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(sparse=False))\n",
    "])\n",
    "\n",
    "# Feature selection (select top 5 features)\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "# Combine pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numerical\", numerical_pipe, numerical_cols),\n",
    "    (\"categorical\", categorical_pipe, categorical_cols),\n",
    "    (\"selector\", selector, features.columns)\n",
    "])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "\n",
    "# Create pipeline with preprocessor and model\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Cross-validation for more robust evaluation\n",
    "cv_scores = cross_val_score(pipeline, features, target, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92000eee-39d7-47a1-b65c-efc15eadd36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population mean estimated with 95% confidence interval: 486.1407070887437 - 513.8592929112564\n"
     ]
    }
   ],
   "source": [
    "Interpretation of Results and Possible Improvements\n",
    "The pipeline automates feature engineering and model building, making the process more efficient.\n",
    "The accuracy of the model on the test dataset indicates how well it generalizes to unseen data.\n",
    "Possible improvements include experimenting with different feature selection methods, imputation strategies, scaling techniques, and models.\n",
    "Additionally, hyperparameter tuning of the Random Forest Classifier could further improve performance. Regularization techniques and handling class imbalance if present in the dataset could also be considered for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a4a3c-0560-4d80-9ea9-e03d6062f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: Building a Pipeline with Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc4bbd02-ab7f-4d68-a52f-aa9f12c7a9bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoting\u001b[39m\u001b[38;5;124m'\u001b[39m, voting_classifier)])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "Here's a response combining the best aspects of Response A and Response B, addressing their shortcomings, and incorporating insights from the ratings to create an enhanced solution for building a pipeline with voting classifiers:\n",
    "\n",
    "Prompt:\n",
    "\n",
    "Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. \n",
    "Train the pipeline on the Iris dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa88c6ee-6cad-4682-9072-9fb780448eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "lr_clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rf_clf), ('lr', lr_clf)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795f237-8ac1-4946-a76a-6aba668a1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interpretation and Improvements:\n",
    "\n",
    "Analyze the accuracy and consider other metrics like precision, recall, and F1-score.\n",
    "Experiment with different voting strategies like \"soft\" voting that weights predictions by model confidence.\n",
    "Try other ensemble methods like gradient boosting or AdaBoost to potentially improve performance.\n",
    "Consider feature engineering techniques like dimensionality reduction or feature selection for larger datasets.\n",
    "Tune hyperparameters of individual classifiers for better accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
